{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erickpython/kodeCamp_5X-MachineLearning/blob/main/Word_Prediction_with_LSTM_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ],
      "metadata": {
        "id": "nb5PKIzHLCnf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmwjL9vGCGmo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Data**"
      ],
      "metadata": {
        "id": "tkLfM6caLHPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"This movie was fantastic I loved it\",\n",
        "    \"Absolutely terrible film waste of time\",\n",
        "    \"Great acting and amazing story\",\n",
        "    \"Worst movie ever\"\n",
        "]"
      ],
      "metadata": {
        "id": "AKGk1AlnCawm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build Vocabulary**\n",
        "\n",
        "We reserve\n",
        "```\n",
        "0 â†’ <PAD>\n",
        "1 â†’ <UNK>\n",
        "```\n",
        "\n",
        "The `tokenize` function splits the provided text into words and converts them to indices into the vocabulary."
      ],
      "metadata": {
        "id": "e7HqsbhLLMqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "counter = Counter()\n",
        "\n",
        "for s in sentences:\n",
        "    counter.update(tokenize(s))\n",
        "\n",
        "word2idx = {\"<PAD>\":0, \"<UNK>\":1}\n",
        "\n",
        "for word in counter:\n",
        "    word2idx[word] = len(word2idx)\n",
        "\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KO2kTlcCmb4",
        "outputId": "2a96de5b-371c-4dda-c86b-de47708fcd65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx, idx2word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BRHJqR94Dc7G",
        "outputId": "f03e7169-ee1d-4050-d560-0e27cddbbf2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'<PAD>': 0,\n",
              "  '<UNK>': 1,\n",
              "  'this': 2,\n",
              "  'movie': 3,\n",
              "  'was': 4,\n",
              "  'fantastic': 5,\n",
              "  'i': 6,\n",
              "  'loved': 7,\n",
              "  'it': 8,\n",
              "  'absolutely': 9,\n",
              "  'terrible': 10,\n",
              "  'film': 11,\n",
              "  'waste': 12,\n",
              "  'of': 13,\n",
              "  'time': 14,\n",
              "  'great': 15,\n",
              "  'acting': 16,\n",
              "  'and': 17,\n",
              "  'amazing': 18,\n",
              "  'story': 19,\n",
              "  'worst': 20,\n",
              "  'ever': 21},\n",
              " {0: '<PAD>',\n",
              "  1: '<UNK>',\n",
              "  2: 'this',\n",
              "  3: 'movie',\n",
              "  4: 'was',\n",
              "  5: 'fantastic',\n",
              "  6: 'i',\n",
              "  7: 'loved',\n",
              "  8: 'it',\n",
              "  9: 'absolutely',\n",
              "  10: 'terrible',\n",
              "  11: 'film',\n",
              "  12: 'waste',\n",
              "  13: 'of',\n",
              "  14: 'time',\n",
              "  15: 'great',\n",
              "  16: 'acting',\n",
              "  17: 'and',\n",
              "  18: 'amazing',\n",
              "  19: 'story',\n",
              "  20: 'worst',\n",
              "  21: 'ever'})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NvTjbelXDJX0",
        "outputId": "9e39c63c-375e-46f0-af9f-e25d6f2572e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'this': 1,\n",
              "         'movie': 2,\n",
              "         'was': 1,\n",
              "         'fantastic': 1,\n",
              "         'i': 1,\n",
              "         'loved': 1,\n",
              "         'it': 1,\n",
              "         'absolutely': 1,\n",
              "         'terrible': 1,\n",
              "         'film': 1,\n",
              "         'waste': 1,\n",
              "         'of': 1,\n",
              "         'time': 1,\n",
              "         'great': 1,\n",
              "         'acting': 1,\n",
              "         'and': 1,\n",
              "         'amazing': 1,\n",
              "         'story': 1,\n",
              "         'worst': 1,\n",
              "         'ever': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Embedding Matrix**\n",
        "\n",
        "Using random vectors here. You can also use pre-trained embeddings from Word2Vec, glove or other sources.\n",
        "\n",
        "### **Word2Vec**\n",
        "```python\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec embedding model.\n",
        "embed_model = Word2Vec(\n",
        "    sentences=tokenized_sentences,\n",
        "    vector_size=100,\n",
        "    window=10,     # context window\n",
        "    min_count=1,\n",
        "    workers=4,\n",
        "    sg=0,          # 1 = skip-gram, 0 = CBOW\n",
        ")\n",
        "\n",
        "# Create word index.\n",
        "word_index = {word: i+1 for i, word in enumerate(embed_model.wv.index_to_key)}\n",
        "\n",
        "# Create embedding matrix.\n",
        "embedding_dim = embed_model.vector_size\n",
        "vocab_size = len(word_index) + 1  # Reserve position 0 for padding.\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_matrix[i] = embed_model.wv[word]\n",
        "```"
      ],
      "metadata": {
        "id": "SElTcgGQLtFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embedding matrix\n",
        "embedding_dim = 50\n",
        "\n",
        "embedding_matrix = np.random.normal(\n",
        "    scale=0.6,\n",
        "    size=(vocab_size, embedding_dim)\n",
        ")\n",
        "\n",
        "embedding_matrix[0] = np.zeros(embedding_dim)  # PAD vector\n",
        "embedding_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeTtWxjPDKiW",
        "outputId": "fa9475b2-1789-4fe9-e3c8-1a0c0c72c11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.12338714,  0.11808445, -0.22969002, ...,  0.19854284,\n",
              "        -0.06849557,  1.07413389],\n",
              "       [ 0.21504217, -0.29340822, -0.23870052, ...,  0.11755211,\n",
              "        -1.24237687,  0.17780739],\n",
              "       ...,\n",
              "       [-0.3254148 , -0.05843513,  0.6612063 , ...,  0.71847021,\n",
              "        -0.05008986,  0.06231694],\n",
              "       [-0.44447929, -0.26648494,  0.81670412, ..., -0.57570844,\n",
              "         0.55597729, -1.12032034],\n",
              "       [ 0.4258125 ,  0.84553726, -1.20692592, ..., -0.44971787,\n",
              "         0.56596023, -0.68565776]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build Next-Word Dataset**\n",
        "\n",
        "We convert:\n",
        "```\n",
        "\"This movie was fantastic\"\n",
        "```\n",
        "\n",
        "into training pairs:\n",
        "```\n",
        "[\"this\"] â†’ movie\n",
        "[\"this movie\"] â†’ was\n",
        "[\"this movie was\"] â†’ fantastic\n",
        "```"
      ],
      "metadata": {
        "id": "4X_RHdePQlQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset Class**"
      ],
      "metadata": {
        "id": "qnkLET4dRTbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NextWordDataset(Dataset):\n",
        "    def __init__(self, sentences, word2idx):\n",
        "        self.samples = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            tokens = tokenize(sentence)\n",
        "            ids = [word2idx.get(t, 1) for t in tokens]\n",
        "\n",
        "            for i in range(1, len(ids)):\n",
        "                context = torch.tensor(ids[:i])\n",
        "                target = ids[i]\n",
        "                self.samples.append((context, target))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n"
      ],
      "metadata": {
        "id": "by3tQWDFEGL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = NextWordDataset(sentences, word2idx)\n",
        "dataset.samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcRB7ls4FMmV",
        "outputId": "1e6a5bd8-9f9f-4e58-8069-f03be90ea64d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([2]), 3),\n",
              " (tensor([2, 3]), 4),\n",
              " (tensor([2, 3, 4]), 5),\n",
              " (tensor([2, 3, 4, 5]), 6),\n",
              " (tensor([2, 3, 4, 5, 6]), 7),\n",
              " (tensor([2, 3, 4, 5, 6, 7]), 8),\n",
              " (tensor([9]), 10),\n",
              " (tensor([ 9, 10]), 11),\n",
              " (tensor([ 9, 10, 11]), 12),\n",
              " (tensor([ 9, 10, 11, 12]), 13),\n",
              " (tensor([ 9, 10, 11, 12, 13]), 14),\n",
              " (tensor([15]), 16),\n",
              " (tensor([15, 16]), 17),\n",
              " (tensor([15, 16, 17]), 18),\n",
              " (tensor([15, 16, 17, 18]), 19),\n",
              " (tensor([20]), 3),\n",
              " (tensor([20,  3]), 21)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Collate Function (Dynamic Padding)**\n",
        "\n",
        "The collate function is where in PyTorch's data loading pipeline you can adjust the data flowing through it. We use this to implement padding of the variable-length sequences using PyTorch's `pad_sequence` function."
      ],
      "metadata": {
        "id": "wfLpEfVURcHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = 0\n",
        "\n",
        "def collate_fn(batch):\n",
        "    contexts = [item[0] for item in batch]\n",
        "    targets = torch.tensor([item[1] for item in batch])\n",
        "\n",
        "    padded_contexts = pad_sequence(\n",
        "        contexts,\n",
        "        batch_first=True,\n",
        "        padding_value=PAD_IDX\n",
        "    )\n",
        "\n",
        "    return padded_contexts, targets\n"
      ],
      "metadata": {
        "id": "FRfa0bjgFP3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collate_fn(dataset[10:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk-vghYMGOQu",
        "outputId": "461258e6-fc0f-4814-e980-006dcdaf4802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 9, 10, 11, 12, 13],\n",
              "         [15,  0,  0,  0,  0],\n",
              "         [15, 16,  0,  0,  0],\n",
              "         [15, 16, 17,  0,  0],\n",
              "         [15, 16, 17, 18,  0]]),\n",
              " tensor([14, 16, 17, 18, 19]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DataLoader**\n",
        "\n",
        "Create a dataloader from an instance of the dataset. The `collate_fn` is passed into the data loader so as to process the data on-the-fly. A batch size of 4 is used. In training with more data, we can use a larger batch size."
      ],
      "metadata": {
        "id": "S77PJoixURLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "S_3yt0xoGQZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Next Word Predictor Model**\n",
        "\n",
        "Using an embedding layer made from the embedding_matrix, and then passing the output into a dense layer. No softmax needed since the cross entropy loss function already includes a softmax."
      ],
      "metadata": {
        "id": "RgBHwDIqUgcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NextWordPredictor(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.tensor(embedding_matrix).float(),\n",
        "            padding_idx=0,\n",
        "            freeze=False\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=128,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        # last timestep\n",
        "        logits = self.fc(output[:, -1, :])\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "iTWmymmHG_br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Loop**\n",
        "\n",
        "A training loop that checks if there is a GPU available, otherwise falls back to the CPU for computations."
      ],
      "metadata": {
        "id": "kuKoh8_GU2f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = NextWordPredictor(embedding_matrix).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "EPOCHS = 200\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    total_loss = 0\n",
        "    mean_loss = 0\n",
        "\n",
        "    for contexts, targets in loader:\n",
        "\n",
        "        contexts = contexts.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        logits = model(contexts)\n",
        "\n",
        "        loss = criterion(logits, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    mean_loss = total_loss / len(loader)\n",
        "    losses.append(mean_loss)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUGTDDbZG3d5",
        "outputId": "b5774577-557c-4a4c-901a-3dbf88e8a487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 15.5523\n",
            "Epoch 20 | Loss: 1.5252\n",
            "Epoch 40 | Loss: 0.4645\n",
            "Epoch 60 | Loss: 0.0417\n",
            "Epoch 80 | Loss: 0.0544\n",
            "Epoch 100 | Loss: 0.0222\n",
            "Epoch 120 | Loss: 0.0135\n",
            "Epoch 140 | Loss: 0.0084\n",
            "Epoch 160 | Loss: 0.0061\n",
            "Epoch 180 | Loss: 0.0057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test the Model (Inference)**"
      ],
      "metadata": {
        "id": "p0JrgdGuVK2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next(model, text, word2idx, idx2word):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    tokens = tokenize(text)\n",
        "    ids = [word2idx.get(t,1) for t in tokens]\n",
        "\n",
        "    x = torch.tensor(ids).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        pred_id = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return idx2word[pred_id]\n",
        "\n",
        "\n",
        "print(predict_next(model, \"this movie was\", word2idx, idx2word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf8-WHXTHsYl",
        "outputId": "1ca1c0c5-53d7-4db7-a6cc-15d6f3727950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fantastic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next(model, \"fantastic movie\", word2idx, idx2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "U6MVrUYnIUgO",
        "outputId": "a3300a34-f534-4e55-876f-ce9cf5f601fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ever'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Insight**\n",
        "\n",
        "This pipeline demonstrates teacher forcing implicitly:\n",
        "\n",
        "The model always receives the true previous tokens, not its own predictions.\n",
        "\n",
        "That is exactly how GPT-style models train.\n",
        "\n",
        "A simplistic generative AI can be obtained from this model by chaining the predictions from the model to create the next input, and allowing the model to keep predicting and generating."
      ],
      "metadata": {
        "id": "lVXOBluWVQkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplistic Generative AI.\n",
        "\n",
        "prompt = \"fantastic movie\"\n",
        "for i in range(20):\n",
        "    next_word = predict_next(model, prompt, word2idx, idx2word)\n",
        "    prompt = prompt + \" \" + next_word\n",
        "    print(prompt)\n",
        "    if next_word == \"<UNK>\":\n",
        "        break\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "wIttX3btJTDB",
        "outputId": "4fd33a20-f510-4ec0-dbd4-5064f3cf28e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fantastic movie ever\n",
            "fantastic movie ever loved\n",
            "fantastic movie ever loved it\n",
            "fantastic movie ever loved it it\n",
            "fantastic movie ever loved it it it\n",
            "fantastic movie ever loved it it it it\n",
            "fantastic movie ever loved it it it it i\n",
            "fantastic movie ever loved it it it it i loved\n",
            "fantastic movie ever loved it it it it i loved it\n",
            "fantastic movie ever loved it it it it i loved it it\n",
            "fantastic movie ever loved it it it it i loved it it movie\n",
            "fantastic movie ever loved it it it it i loved it it movie ever\n",
            "fantastic movie ever loved it it it it i loved it it movie ever ever\n",
            "fantastic movie ever loved it it it it i loved it it movie ever ever ever\n",
            "fantastic movie ever loved it it it it i loved it it movie ever ever ever ever\n",
            "fantastic movie ever loved it it it it i loved it it movie ever ever ever ever ever\n",
            "fantastic movie ever loved it it it it i loved it it movie ever ever ever ever ever ever\n",
            "fantastic movie ever loved it it it it i loved it it movie ever ever ever ever ever ever ever\n",
            "fantastic movie ever loved it it it it i loved it it movie ever ever ever ever ever ever ever ever\n",
            "fantastic movie ever loved it it it it i loved it it movie ever ever ever ever ever ever ever ever ever\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fantastic movie ever loved it it it it i loved it it movie ever ever ever ever ever ever ever ever ever'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, our model is too simplistic and was trained on too little data to generate any meaningful sentences.\n",
        "\n",
        "With only 4 sentences:\n",
        "\n",
        "ðŸ‘‰ The model is learning memorization, not language.\n",
        "\n",
        "For a real demo, give it:\n",
        "\n",
        "- IMDb\n",
        "- WikiText-2\n",
        "- Tiny Shakespeare\n",
        "\n",
        "Then you will see real emergence."
      ],
      "metadata": {
        "id": "SbLi43xxVq0g"
      }
    }
  ]
}